{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing Anime dataset from Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from scipy.sparse import csr_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import string\n",
    "import itertools\n",
    "import statistics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Data preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1 Data Cleaning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anime data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime_df = pd.read_csv(\"datasets/animes.csv\")\n",
    "\n",
    "anime_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing values + duplicate data for animes.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_value = pd.DataFrame({\n",
    "    'Missing Value': anime_df.isnull().sum()\n",
    "})\n",
    "display(missing_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate = anime_df.duplicated(subset=['title']).sum()\n",
    "print('There are {} duplicated rows in anime_df'.format(duplicate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove unwanted features (columns) from the dataset\n",
    "anime_df.rename(columns={'title': 'name'}, inplace=True)\n",
    "anime_df.drop(['synopsis', 'aired', 'ranked', 'img_url', 'link'], axis=1, inplace=True)\n",
    "\n",
    "#removing unwanted characters from the anime name strings\n",
    "def text_cleaning(text):\n",
    "    text = re.sub(r'&quot;', '', text)\n",
    "    text  = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    text = re.sub(r'.hack//', '', text)\n",
    "    text = re.sub(r'&#039;', '', text)\n",
    "    text = re.sub(r'A&#039;s', '', text)\n",
    "    text = re.sub(r'I&#039;', 'I\\'', text)\n",
    "    text = re.sub(r'&amp;', 'and', text)\n",
    "    text = re.sub(r'Â°', '',text)\n",
    "\n",
    "    return text\n",
    "\n",
    "anime_df['name'] = anime_df['name'].apply(text_cleaning)\n",
    "anime_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime_df.rename(columns={'uid': 'anime_id', 'score': 'rating'}, inplace=True)\n",
    "anime_df.episodes.replace({'Unknown':np.nan},inplace=True)\n",
    "\n",
    "anime_df.drop_duplicates(subset=['name'], inplace=True)\n",
    "anime_df.dropna(inplace=True)\n",
    "anime_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "anime_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace the characters \"[]'\" with an empty space as the genre column is already of type string\n",
    "anime_df['genre'] = anime_df['genre'].str.replace(\"'\", \"\", regex=False)\n",
    "anime_df['genre'] = anime_df['genre'].str.replace(\"[\", \"\", regex=False)\n",
    "anime_df['genre'] = anime_df['genre'].str.replace(\"]\", \"\", regex=False)\n",
    "\n",
    "anime_df.head(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ratings data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ratings_df = pd.read_csv(\"datasets/reviews.csv\")\n",
    "\n",
    "user_ratings_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing values + duplicate data for animes.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_value = pd.DataFrame({\n",
    "    'Missing Value': user_ratings_df.isnull().sum()\n",
    "})\n",
    "display(missing_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ratings_df.drop(['link', 'text'], axis=1, inplace=True)\n",
    "user_ratings_df.rename(columns={'profile': 'user_id'}, inplace=True)\n",
    "\n",
    "user_ratings_df.user_id = pd.factorize(user_ratings_df.user_id)[0]\n",
    "\n",
    "user_ratings_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ratings_df['scores'] = user_ratings_df['scores'].str.replace(\"'\", \"\", regex=False)\n",
    "user_ratings_df['scores'] = user_ratings_df['scores'].str.replace(\"{\", \"\", regex=False)\n",
    "user_ratings_df['scores'] = user_ratings_df['scores'].str.replace(\"}\", \"\", regex=False)\n",
    "\n",
    "user_ratings_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ratings_df['scores'] = [re.sub(\"[^0-9,]\", \"\", anime) for anime in user_ratings_df['scores']]\n",
    "user_ratings_df.rename(columns={\"anime_uid\": \"anime_id\"}, inplace=True)\n",
    "\n",
    "user_ratings_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_ratings_df = user_ratings_df['scores'].str.split(\",\", expand=True)\n",
    "category_ratings_df.columns = ['Overall', 'Story', 'Animation','Sound', 'Character', 'Enjoyment']\n",
    "\n",
    "category_ratings_df.head(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalised user_ratings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ratings_df = pd.concat([user_ratings_df, category_ratings_df], axis=1)\n",
    "user_ratings_df.drop(columns=['score', 'scores', 'uid'], inplace=True)\n",
    "\n",
    "user_ratings_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ratings_df[['Overall', 'Story', 'Animation', 'Sound', 'Character', 'Enjoyment']] = user_ratings_df[\n",
    "    ['Overall', 'Story', 'Animation', 'Sound', 'Character', 'Enjoyment']].apply(pd.to_numeric)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate = user_ratings_df.duplicated(subset=['user_id', 'anime_id']).sum()\n",
    "print('There are {} duplicated rows in user_ratings_df'.format(duplicate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ratings_df.drop_duplicates(subset=['user_id', 'anime_id'], inplace=True)\n",
    "\n",
    "duplicate = user_ratings_df.duplicated(subset=['user_id', 'anime_id']).sum()\n",
    "print('There are {} duplicated rows in user_ratings_df'.format(duplicate))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merging the anime and ratings dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime_with_ratings_df = pd.merge(anime_df, user_ratings_df, on='anime_id')\n",
    "\n",
    "anime_with_ratings_df.drop_duplicates(subset=['user_id', 'name'], inplace=True)\n",
    "anime_with_ratings_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "anime_with_ratings_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Content based filtering recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use MinMax normalisation to normalise the values of each feature, multiplied by their \n",
    "# corresponding weight\n",
    "normalised_anime_df = anime_df.copy()\n",
    "\n",
    "weights = {\n",
    "    'genre': 0.3,\n",
    "    'members_norm': 0.1,\n",
    "    'rating_norm': 0.4,\n",
    "    'popularity_norm': 0.1,\n",
    "    'episodes_norm': 0.1\n",
    "}\n",
    "\n",
    "members_min_val = normalised_anime_df['members'].min()\n",
    "members_max_val = normalised_anime_df['members'].max()\n",
    "\n",
    "ratings_min_val = normalised_anime_df['rating'].min()\n",
    "ratings_max_val = normalised_anime_df['rating'].max()\n",
    "\n",
    "popularity_min_val = normalised_anime_df['popularity'].min()\n",
    "popularity_max_val = normalised_anime_df['popularity'].max()\n",
    "\n",
    "episodes_min_val = normalised_anime_df['episodes'].min()\n",
    "episodes_max_val = normalised_anime_df['episodes'].max()\n",
    "\n",
    "normalised_anime_df['members_norm'] = (normalised_anime_df['members'] - members_min_val) / (members_max_val - members_min_val) * weights['members_norm']\n",
    "normalised_anime_df['avg_rating_norm'] = (normalised_anime_df['rating'] - ratings_min_val) / (ratings_max_val - ratings_min_val) * weights['rating_norm']\n",
    "normalised_anime_df['popularity_norm'] = (normalised_anime_df['popularity'] - popularity_min_val) / (popularity_max_val - popularity_min_val) * weights['popularity_norm']\n",
    "normalised_anime_df['episodes_norm'] = (normalised_anime_df['episodes'] - episodes_min_val) / (episodes_max_val - episodes_min_val) * weights['episodes_norm']\n",
    "\n",
    "normalised_anime_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalised_anime_df.drop(['members', 'rating', 'popularity', 'episodes'], axis=1, inplace=True)\n",
    "\n",
    "normalised_anime_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genres_df = anime_df['genre'].str.get_dummies(sep=', ').astype(int)\n",
    "genres_df = genres_df.apply(lambda x : x * weights['genre'])\n",
    "\n",
    "genres_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalised_anime_df.drop('genre', axis=1, inplace=True)\n",
    "normalised_anime_df = pd.concat([normalised_anime_df, genres_df], axis=1)\n",
    "\n",
    "normalised_anime_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recommend one season of a show.\n",
    "\n",
    "E.g. if the recommendations have \"Tokyo ghoul season 1, tokyo ghoul season 2\" etc. we want to only recommend one of these.\n",
    "\n",
    "Recommend the one with the highest average rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_recommendations(anime_titles, anime_df=anime_df):\n",
    "    titles = anime_titles[:]\n",
    "\n",
    "    # sort all the anime titles so that it is easier to group similar titles\n",
    "    titles.sort()\n",
    "    iterator = itertools.groupby(titles, lambda string: string.split(' ')[0])\n",
    "\n",
    "    grouped_titles = []\n",
    "    for element, group in iterator:\n",
    "        grouped_titles.append(list(group))\n",
    "\n",
    "    # checking for each grouped anime title which one has the highest average rating so that we can recommend\n",
    "    # that one to the user\n",
    "    unique_titles = []\n",
    "    title = ''\n",
    "    for anime_group in grouped_titles:\n",
    "        max_rating = 0\n",
    "        for anime in anime_group:\n",
    "            anime_index = anime_df[anime_df['name'] == anime].index\n",
    "            curr_rating = anime_df['rating'].iloc[anime_index[0]]\n",
    "\n",
    "            if curr_rating > max_rating:\n",
    "                max_rating = anime_df['rating'].iloc[anime_index[0]]\n",
    "                title = anime_df['name'].iloc[anime_index[0]]\n",
    "\n",
    "        unique_titles.append(title)\n",
    "\n",
    "    return unique_titles"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Content-based filtering -> Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['members_norm', 'avg_rating_norm', 'popularity_norm', 'episodes_norm'] + genres_df.columns.tolist()\n",
    "\n",
    "cosine_sim = cosine_similarity(normalised_anime_df[features], normalised_anime_df[features])\n",
    "\n",
    "print(cosine_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = pd.Series(anime_df.index, index=anime_df['name']).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def content_based_recommendations(title, cosine_sim=cosine_sim, anime_df=anime_df, indices=indices, n_recommendations=50):\n",
    "    # Get the index of the anime that matches the title\n",
    "    index = indices[title]\n",
    "    \n",
    "    # Get the pairwise cosine similarity scores for all anime with that index\n",
    "    sim_scores = list(enumerate(cosine_sim[index]))\n",
    "\n",
    "    # Sort the anime based on the similarity scores\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Get the top 100 most similar anime -> allows us to have more anime so that we are \n",
    "    # still able to recommend n_recommendations animes to the user after getting all the unique titles\n",
    "    sim_scores = sim_scores[1:101]\n",
    "\n",
    "    # Get the titles of the top 10 most similar anime\n",
    "    anime_indices = [i[0] for i in sim_scores]\n",
    "    \n",
    "    anime_titles = anime_df['name'].iloc[anime_indices].values.tolist()\n",
    "\n",
    "    # convert to set for constant lookup time\n",
    "    unique_titles = set(get_unique_recommendations(anime_titles))\n",
    "\n",
    "    recommendations = [i for i in anime_titles if i in unique_titles]\n",
    "\n",
    "    return recommendations[:n_recommendations+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_based_recommendations('Death Note')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Content-based filtering -> Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "clustering_features = normalised_anime_df[features].copy()\n",
    "\n",
    "# ========compute the number of clusters to use========\n",
    "\n",
    "sse = [] #sum of squared errors\n",
    "\n",
    "# silhouette scores range from -1 to 1: \n",
    "# 1 = points are perfectly assigned in a clsuter and clusters are easily distinguishable\n",
    "# 0 = clusters are overlapping\n",
    "# -1 = points are wrongly assigned in a cluster\n",
    "\n",
    "silhouette_coefficients = []\n",
    "\n",
    "for k in range(2, 11):\n",
    "    kmeans = KMeans(n_clusters = k, n_init = 10) #max_iter = 100\n",
    "    kmeans.fit(clustering_features)\n",
    "    sse.append(kmeans.inertia_)\n",
    "\n",
    "    ss = silhouette_score(clustering_features, kmeans.labels_)\n",
    "    silhouette_coefficients.append(ss)\n",
    "\n",
    "print(\"SSE: \", sse)\n",
    "print(\"Silhouette scores: \", silhouette_coefficients)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(2, 11), sse)\n",
    "plt.xticks(range(2, 11))\n",
    "plt.xlabel(\"Number of Clusters\")\n",
    "plt.ylabel(\"SSE\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Silhouette coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(2, 11), silhouette_coefficients)\n",
    "plt.xticks(range(2, 11))\n",
    "plt.xlabel(\"Number of Clusters\")\n",
    "plt.ylabel(\"Silhouette coefficients\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we label each anime to its designated cluster which will be used to generate recommendations\n",
    "kmeans = KMeans(n_clusters=4, random_state=42, n_init=10)\n",
    "anime_clusters = kmeans.fit_predict(clustering_features)\n",
    "anime_clusters_visual = kmeans.fit(clustering_features)\n",
    "\n",
    "anime_with_clusters = anime_df.copy()\n",
    "anime_with_clusters['Cluster'] = anime_clusters_visual.labels_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify the 2 features that have the highest correlation with each other to be used in the cluster plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = clustering_features.corr(numeric_only=True)\n",
    "\n",
    "# Set diagonal values to 0 as these indicate the correlation between the same features, i.e. members_norm, members_norm\n",
    "np.fill_diagonal(corr_matrix.values, 0)\n",
    "\n",
    "print(corr_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=4, random_state=42, n_init=10)\n",
    "kmeans.fit(clustering_features)\n",
    "\n",
    "labels = kmeans.labels_\n",
    "\n",
    "most_correlated = corr_matrix.abs().unstack().sort_values(ascending=False).drop_duplicates()\n",
    "high_corr_pairs = most_correlated[most_correlated != 1][:2].reset_index()\n",
    "\n",
    "feature1 = high_corr_pairs.iloc[0]['level_0']\n",
    "feature2 = high_corr_pairs.iloc[0]['level_1']\n",
    "\n",
    "print(f'Top pair of features: {feature1}, {feature2}')\n",
    "print(f'Correlation coefficient: {corr_matrix.loc[feature1, feature2]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_plot(data, nclusters):\n",
    "    corr_matrix = data.corr(numeric_only=True)\n",
    "\n",
    "    # Set diagonal values to 0 as these indicate the correlation between the same features, i.e. members_norm, members_norm\n",
    "    np.fill_diagonal(corr_matrix.values, 0)\n",
    "\n",
    "    kmeans = KMeans(n_clusters=4, random_state=42, n_init=10)\n",
    "    kmeans.fit(data)\n",
    "\n",
    "    labels = kmeans.labels_\n",
    "\n",
    "    most_correlated = corr_matrix.abs().unstack().sort_values(ascending=False).drop_duplicates()\n",
    "    high_corr_pairs = most_correlated[most_correlated != 1][:2].reset_index()\n",
    "\n",
    "    feature1 = high_corr_pairs.iloc[0]['level_0']\n",
    "    feature2 = high_corr_pairs.iloc[0]['level_1']\n",
    "\n",
    "\n",
    "    X = clustering_features[[feature1, feature2]].values\n",
    "\n",
    "    kmeans = KMeans(n_clusters=4, random_state=42, n_init=10).fit(X)\n",
    "\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=kmeans.labels_)\n",
    "    plt.xlabel(feature1)\n",
    "    plt.ylabel(feature2)\n",
    "    plt.title('Cluster plot of highest correlated features')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = clustering_features[[feature1, feature2]].values\n",
    "kmeans = KMeans(n_clusters=4, random_state=42, n_init=10).fit(X)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=kmeans.labels_)\n",
    "plt.xlabel(feature1)\n",
    "plt.ylabel(feature2)\n",
    "plt.title('Cluster plot of highest correlated features')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similar_anime(anime_name, data, n_recommendations=100):\n",
    "    anime_index = anime_df[anime_df['name'] == anime_name].index[0]\n",
    "    anime_cluster = data[anime_index]\n",
    "    similar_anime_indexes = [i for i, cluster in enumerate(data) if cluster == anime_cluster and i != anime_index]\n",
    "    similar_anime = anime_df.iloc[similar_anime_indexes]['name'].tolist()\n",
    "\n",
    "    unique_titles = set(get_unique_recommendations(similar_anime))\n",
    "    \n",
    "    recommendations = [i for i in similar_anime if i in unique_titles]\n",
    "    \n",
    "    return recommendations[:n_recommendations+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_similar_anime('Death Note', anime_clusters)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering With PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['members_norm', 'avg_rating_norm', 'popularity_norm', 'episodes_norm'] + genres_df.columns.tolist()\n",
    "\n",
    "selected_features_pca = normalised_anime_df[features].copy()\n",
    "pca_anime_df = normalised_anime_df[features].copy()\n",
    "\n",
    "pca = PCA().fit(pca_anime_df)\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "cumulative_variance = explained_variance.cumsum()\n",
    "\n",
    "plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance)\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=30)\n",
    "pca_result = pca.fit_transform(pca_anime_df)\n",
    "\n",
    "# compute the number of clusters to use\n",
    "sse = [] #sum of squared errors\n",
    "\n",
    "# silhouette scores range from -1 to 1: \n",
    "# 1 = points are perfectly assigned in a clsuter and clusters are easily distinguishable\n",
    "# 0 = clusters are overlapping\n",
    "# -1 = points are wrongly assigned in a cluster\n",
    "\n",
    "silhouette_coefficients = []\n",
    "\n",
    "for k in range(2, 11):\n",
    "    kmeans = KMeans(n_clusters = k, n_init = 10) #max_iter = 100\n",
    "    kmeans.fit(pca_result)\n",
    "    sse.append(kmeans.inertia_)\n",
    "\n",
    "    ss = silhouette_score(pca_result, kmeans.labels_)\n",
    "    silhouette_coefficients.append(ss)\n",
    "\n",
    "print(\"SSE: \", sse)\n",
    "print(\"Silhouette scores: \", silhouette_coefficients)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SSE With PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(2, 11), sse)\n",
    "plt.xticks(range(2, 11))\n",
    "plt.xlabel(\"Number of Clusters\")\n",
    "plt.ylabel(\"SSE\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Silhouette Coefficients With PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(2, 11), silhouette_coefficients)\n",
    "plt.xticks(range(2, 11))\n",
    "plt.xlabel(\"Number of Clusters\")\n",
    "plt.ylabel(\"Silhouette coefficients\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_cluster_plot(data, nclusters):\n",
    "    import matplotlib.pyplot as plt\n",
    "    from sklearn.cluster import KMeans\n",
    "\n",
    "    pca = PCA(n_components=2)\n",
    "    X_pca = pca.fit_transform(data)\n",
    "\n",
    "    kmeans = KMeans(n_clusters=nclusters, random_state=42, n_init=10)\n",
    "    kmeans.fit(X_pca)\n",
    "    labels = kmeans.labels_\n",
    "\n",
    "    plt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels)\n",
    "    plt.xlabel('Principal Component 1')\n",
    "    plt.ylabel('Principal Component 2')\n",
    "    plt.title('K-means Clustering with PCA using the top 2 Principal Components')\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_pd = pd.DataFrame(pca_result)\n",
    "pca_cluster_plot(pca_pd, 4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = clustering_features\n",
    "cluster_plot(clusters, 4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get recommendations using PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=4, random_state=42, n_init=10)\n",
    "kmeans.fit(pca_result)\n",
    "anime_clusters_after_pca = kmeans.predict(pca_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_similar_anime('Death Note', anime_clusters_after_pca)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Item-item Collaborative filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime_with_ratings_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pivot_table(data, value):\n",
    "    pivot_table = data.pivot_table(index='user_id', columns='name',values=value)\n",
    "    pivot_table.fillna(0, inplace=True)\n",
    "\n",
    "    return pivot_table"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create user-item matrices (pivot tables) for each rating category (story, animation, etc.) containing the rating value given by users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 55.3seconds to execute with all categories\n",
    "overall_pivot = create_pivot_table(anime_with_ratings_df, 'Overall')\n",
    "story_pivot = create_pivot_table(anime_with_ratings_df, 'Story')\n",
    "animation_pivot = create_pivot_table(anime_with_ratings_df, 'Animation')\n",
    "character_pivot = create_pivot_table(anime_with_ratings_df, 'Character')\n",
    "\n",
    "overall_pivot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_similarities(pivot_table):\n",
    "    sparse_pivot = csr_matrix(pivot_table)\n",
    "    similarities = cosine_similarity(sparse_pivot.T)\n",
    "    similarities_df = pd.DataFrame(similarities, index=pivot_table.columns, columns=pivot_table.columns)\n",
    "\n",
    "    return similarities_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Cosine Similarity to calculate the similarity between each anime using the different categories rated by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_similarities_df = calculate_similarities(overall_pivot)\n",
    "story_similarities_df = calculate_similarities(story_pivot)\n",
    "animation_similarities_df = calculate_similarities(animation_pivot)\n",
    "character_similarities_df = calculate_similarities(character_pivot)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine the values in the dataframes obtained above using the assigned weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_weight = 0.5 \n",
    "story_weight=0.3\n",
    "animation_weight=0.1\n",
    "character_weight=0.1\n",
    "\n",
    "combined_category_ratings_pivot = (overall_similarities_df * overall_weight) + (story_similarities_df * story_weight) + (animation_similarities_df * animation_weight) \n",
    "+ (character_similarities_df * character_weight) \n",
    "\n",
    "combined_category_ratings_pivot.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get collaborative filtering recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collaborative_filtering_recommendations(anime, n=50):\n",
    "    similarity_scores = combined_category_ratings_pivot[anime]\n",
    "    similarity_scores = similarity_scores.sort_values(ascending=False)\n",
    "\n",
    "    similar_anime = similarity_scores.iloc[1:n+1].index.tolist()\n",
    "\n",
    "    unique_titles = set(get_unique_recommendations(similar_anime))\n",
    "\n",
    "    recommendations = [i for i in similar_anime if i in unique_titles]\n",
    "\n",
    "    return recommendations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collaborative_filtering_recommendations('Death Note')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hybrid Implementation: Combine content based and collaborative filtering methods to provide recommendations to the user"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Content-based filtering component using Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_recommendations(anime_name, content_weight=0.5, collaborative_weight=0.5, num_recommendations=20):\n",
    "    if anime_name not in combined_category_ratings_pivot.index:\n",
    "        return []\n",
    "\n",
    "    content_based = content_based_recommendations(anime_name)\n",
    "    collaborative_filtering = collaborative_filtering_recommendations(anime_name)\n",
    "\n",
    "    # removing anime titles that may no longer exist within our dataframe as some were removed after the initial\n",
    "    # anime_df and ratings_df dataframes were merged together\n",
    "    content_based_animes = []\n",
    "\n",
    "    for i in content_based:\n",
    "        if i in combined_category_ratings_pivot.index:\n",
    "            content_based_animes.append(i)\n",
    "\n",
    "    collaborative_based_animes = []\n",
    "\n",
    "    for i in collaborative_filtering:\n",
    "        if i in combined_category_ratings_pivot.index:\n",
    "            collaborative_based_animes.append(i)\n",
    "\n",
    "    content_based_scores = combined_category_ratings_pivot.loc[content_based_animes]\n",
    "    collaborative_filtering_scores = combined_category_ratings_pivot.loc[collaborative_based_animes]\n",
    "\n",
    "    # create weighted scores for all the animes using both the values generated from content based and collaborative filtering methods\n",
    "    scores = content_based_scores.mul(content_weight).add(collaborative_filtering_scores.mul(collaborative_weight), fill_value=0)\n",
    "    \n",
    "    weighted_scores = scores[anime_name].sort_values(ascending=False)\n",
    "    return weighted_scores, weighted_scores.head(num_recommendations).index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_recommendations('Death Note')[1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Content-based filtering component using K-means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_recommendations_with_clustering(anime_name, content_weight=0.5, collaborative_weight=0.5, num_recommendations=20):\n",
    "    if anime_name not in combined_category_ratings_pivot.index:\n",
    "        return []\n",
    "\n",
    "    content_based = get_similar_anime(anime_name, anime_clusters)\n",
    "    collaborative_filtering = collaborative_filtering_recommendations(anime_name)\n",
    "\n",
    "    # removing anime titles that may no longer exist within our dataframe as some were removed after the initial\n",
    "    # anime_df and ratings_df dataframes were merged together\n",
    "    content_based_animes = []\n",
    "\n",
    "    for i in content_based:\n",
    "        if i in combined_category_ratings_pivot.index:\n",
    "            content_based_animes.append(i)\n",
    "\n",
    "    collaborative_based_animes = []\n",
    "\n",
    "    for i in collaborative_filtering:\n",
    "        if i in combined_category_ratings_pivot.index:\n",
    "            collaborative_based_animes.append(i)\n",
    "\n",
    "    content_based_scores = combined_category_ratings_pivot.loc[content_based_animes]\n",
    "    collaborative_filtering_scores = combined_category_ratings_pivot.loc[collaborative_based_animes]\n",
    "\n",
    "    # create weighted scores for all the animes using both the values generated from content based and collaborative filtering methods\n",
    "    scores = content_based_scores.mul(content_weight).add(collaborative_filtering_scores.mul(collaborative_weight), fill_value=0)\n",
    "    \n",
    "    weighted_scores = scores[anime_name].sort_values(ascending=False)\n",
    "    return weighted_scores, weighted_scores.head(num_recommendations).index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_recommendations_with_clustering('Death Note')[1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Content-based filtering component using K-Means Clustering with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_recommendations_with_clustering_and_pca(anime_name, content_weight=0.5, collaborative_weight=0.5, num_recommendations=20):\n",
    "    if anime_name not in combined_category_ratings_pivot.index:\n",
    "        return []\n",
    "\n",
    "    content_based = get_similar_anime(anime_name, anime_clusters_after_pca)\n",
    "    collaborative_filtering = collaborative_filtering_recommendations(anime_name)\n",
    "\n",
    "    # removing anime titles that may no longer exist within our dataframe as some were removed after the initial\n",
    "    # anime_df and ratings_df dataframes were merged together\n",
    "    content_based_animes = []\n",
    "\n",
    "    for i in content_based:\n",
    "        if i in combined_category_ratings_pivot.index:\n",
    "            content_based_animes.append(i)\n",
    "\n",
    "    collaborative_based_animes = []\n",
    "\n",
    "    for i in collaborative_filtering:\n",
    "        if i in combined_category_ratings_pivot.index:\n",
    "            collaborative_based_animes.append(i)\n",
    "\n",
    "    content_based_scores = combined_category_ratings_pivot.loc[content_based_animes]\n",
    "    collaborative_filtering_scores = combined_category_ratings_pivot.loc[collaborative_based_animes]\n",
    "\n",
    "    # create weighted scores for all the animes using both the values generated from content based and collaborative filtering methods\n",
    "    scores = content_based_scores.mul(content_weight).add(collaborative_filtering_scores.mul(collaborative_weight), fill_value=0)\n",
    "    \n",
    "    weighted_scores = scores[anime_name].sort_values(ascending=False)\n",
    "    return weighted_scores, weighted_scores.head(num_recommendations).index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_recommendations_with_clustering_and_pca('Death Note')[1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAE (Mean Absolute Error)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dataframe to store the mean and standard deviation of all users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_and_std(user_id):\n",
    "    user = user_ratings_df[user_ratings_df['user_id'] == user_id]\n",
    "\n",
    "    user_mean = user['Overall'].mean()\n",
    "    user_std = user['Overall'].std()\n",
    "\n",
    "    return user_mean, user_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each user id in user_ratings_df['user_id'].unique(), get the mean \n",
    "# and standard deviation for that user\n",
    "\n",
    "# create a dataframe from the results: user_id as index, mean and std as columns\n",
    "\n",
    "user_mean_std = [get_mean_and_std(user_id) for user_id in user_ratings_df['user_id'].unique()]\n",
    "\n",
    "user_mean_std_df = pd.DataFrame(user_mean_std, columns=['mean', 'std'])\n",
    "\n",
    "user_mean_std_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mae(recommendations):\n",
    "    # Create pivot table with users as rows and recommended anime as columns\n",
    "    recommendations_pivot = recommendations.pivot_table(index='user_id', columns='anime_id', values='hybrid_score')\n",
    "    \n",
    "    # Merge pivot table with actual ratings data to get ratings for recommended anime\n",
    "    ratings = pd.merge(mae_df, recommendations_pivot.stack().reset_index().rename(columns={0: 'hybrid_score'}), on=['user_id', 'anime_id'])\n",
    "    ratings.drop(columns={'hybrid_score_x'}, inplace=True)\n",
    "    ratings.rename(columns={'hybrid_score_y' : 'hybrid_score'}, inplace=True)\n",
    "    ratings.drop_duplicates(subset=['user_id', 'anime_id'], inplace=True) \n",
    "\n",
    "    if not ratings['normalized_rating'].shape[0] > 1 or not ratings['hybrid_score'].shape[0] > 1:\n",
    "        return None\n",
    "        \n",
    "    # # Calculate MAE\n",
    "    mae = mean_absolute_error(ratings['normalized_rating'], ratings['hybrid_score'])\n",
    "    return mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mae_value_without_clustering(title):\n",
    "    \n",
    "    # get the similarity scores of the hybrid recommendation titles\n",
    "    hybrid_recommendations_scores = combined_recommendations(title)[0]\n",
    "    hybrid_recs_df = hybrid_recommendations_scores.to_frame(name='hybrid_score')\n",
    "\n",
    "    get_hybrid_recs_df(title)\n",
    "\n",
    "    # Add anime ids to each of the recommendations so we can merge with anime_with_ratings_df \n",
    "    anime_indexes = [anime_df[anime_df['name'] == anime_name].index[0] for anime_name in hybrid_recs_df.index]\n",
    "    anime_ids = [anime_df.loc[index, 'anime_id'] for index in anime_indexes]\n",
    "    hybrid_recs_df['anime_id'] = anime_ids\n",
    "\n",
    "    # Create our mae_df to be used specifically for MAE calculations\n",
    "    mae_df = anime_with_ratings_df.copy()\n",
    "    mae_df= pd.merge(anime_with_ratings_df, hybrid_recs_df, on='anime_id')\n",
    "    mae_df[['Overall', 'Story', 'Animation', 'Character']] = mae_df[\n",
    "    ['Overall', 'Story', 'Animation', 'Character']].apply(pd.to_numeric)\n",
    "\n",
    "    # Add 'normalized_rating' column, which uses the Overall score a user has given an anime \n",
    "    # alongside the mean and standard deviation of all the animes a user has rated to normalise.\n",
    "    mae_df['normalized_rating'] = mae_df.apply((lambda row: \n",
    "    (row['Overall'] - user_mean_std_df.loc[row['user_id'], 'mean']) / user_mean_std_df.loc[row['user_id'], 'std'] \n",
    "    if user_mean_std_df.loc[row['user_id'], 'std'] != 0 else 0), axis=1)\n",
    "    \n",
    "    # # # NaN values are present as the user has only rated this single anime, so they \n",
    "    # # # are not that useful when providing recommendations, so we can simply just fill it with 0.\n",
    "    mae_df['normalized_rating'] = mae_df['normalized_rating'].fillna(0)\n",
    "\n",
    "    mae = calculate_mae(mae_df)\n",
    "\n",
    "    return mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mae_value_with_clustering(title):\n",
    "\n",
    "    hybrid_recommendations_scores = combined_recommendations_with_clustering(title)[0]\n",
    "    hybrid_recs_df = hybrid_recommendations_scores.to_frame(name='hybrid_score')\n",
    "\n",
    "    anime_indexes = [anime_df[anime_df['name'] == anime_name].index[0] for anime_name in hybrid_recs_df.index]\n",
    "    anime_ids = [anime_df.loc[index, 'anime_id'] for index in anime_indexes]\n",
    "\n",
    "    hybrid_recs_df['anime_id'] = anime_ids\n",
    "\n",
    "    mae_df = anime_with_ratings_df.copy()\n",
    "    mae_df = pd.merge(anime_with_ratings_df, hybrid_recs_df, on='anime_id')\n",
    "\n",
    "    mae_df[['Overall', 'Story', 'Animation', 'Character']] = mae_df[\n",
    "        ['Overall', 'Story', 'Animation', 'Character']].apply(pd.to_numeric)\n",
    "\n",
    "    mae_df['normalized_rating'] = mae_df.apply((lambda row: \n",
    "    (row['Overall'] - user_mean_std_df.loc[row['user_id'], 'mean']) / user_mean_std_df.loc[row['user_id'], 'std'] \n",
    "    if user_mean_std_df.loc[row['user_id'], 'std'] != 0 else 0), axis=1)\n",
    "    \n",
    "    mae_df['normalized_rating'] = mae_df['normalized_rating'].fillna(0)\n",
    "\n",
    "    mae = calculate_mae(mae_df)\n",
    "\n",
    "    return mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mae_value_with_clustering_and_pca(title):\n",
    "\n",
    "    hybrid_recomhybrid_recommendations_scoresmendations = combined_recommendations_with_clustering_and_pca(title)[0]\n",
    "    hybrid_recs_df = hybrid_recommendations_scores.to_frame(name='hybrid_score')\n",
    "\n",
    "    anime_indexes = [anime_df[anime_df['name'] == anime_name].index[0] for anime_name in hybrid_recs_df.index]\n",
    "    anime_ids = [anime_df.loc[index, 'anime_id'] for index in anime_indexes]\n",
    "\n",
    "    hybrid_recs_df['anime_id'] = anime_ids\n",
    "\n",
    "    mae_df = anime_with_ratings_df.copy()\n",
    "    mae_df = pd.merge(anime_with_ratings_df, hybrid_recs_df, on='anime_id')\n",
    "\n",
    "    mae_df[['Overall', 'Story', 'Animation', 'Character']] = mae_df[\n",
    "    ['Overall', 'Story', 'Animation', 'Character']].apply(pd.to_numeric)\n",
    "\n",
    "    mae_df['normalized_rating'] = mae_df.apply((lambda row: \n",
    "    (row['Overall'] - user_mean_std_df.loc[row['user_id'], 'mean']) / user_mean_std_df.loc[row['user_id'], 'std'] \n",
    "    if user_mean_std_df.loc[row['user_id'], 'std'] != 0 else 0), axis=1)\n",
    "    \n",
    "    mae_df['normalized_rating'] = mae_df['normalized_rating'].fillna(0)\n",
    "\n",
    "\n",
    "    mae = calculate_mae(mae_df)\n",
    "\n",
    "    return mae"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating the MAE for every anime title would take a very long time, so we limit the number of titles to save us time."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAE for Cosine Similarity Content-based filtering approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_values_without_clustering = [calculate_mae(mae_value_without_clustering(title)) for title in anime_with_ratings_df['name'].unique()[:100]]\n",
    "mae_values_without_clustering = [i for i in mae_values_without_clustering if i is not None]\n",
    "\n",
    "mae_mean_without_clustering = statistics.mean(mae_values_without_clustering)\n",
    "\n",
    "mae_mean_without_clustering"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAE for Clustering without PCA Content-based filtering approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_values_with_clustering = [mae_value_with_clustering(title) for title in anime_with_ratings_df['name'].unique()[:100]]\n",
    "mae_values_with_clustering = [i for i in mae_values_with_clustering if i is not None]\n",
    "\n",
    "mae_mean_with_clustering = statistics.mean(mae_values_with_clustering)\n",
    "\n",
    "mae_mean_with_clustering"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAE for Clustering with PCA Content-based filtering approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_values_with_pca = [mae_value_with_clustering_and_pca(title) for title in anime_with_ratings_df['name'].unique()[:100]]\n",
    "mae_values_with_pca = [i for i in mae_values_with_pca if i is not None]\n",
    "\n",
    "mae_mean_with_pca = statistics.mean(mae_values_with_pca)\n",
    "\n",
    "mae_mean_with_pca"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coverage Testing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coverage Testing -> Cosine Similarity Content-based filtering approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_anime_titles = list(anime_with_ratings_df['name'].unique())\n",
    "\n",
    "# set to store all recommended anime titles\n",
    "recommended_anime = set()\n",
    "\n",
    "for anime_title in all_anime_titles:\n",
    "    recommendations = combined_recommendations(anime_title)[1]\n",
    "    for recommendation in recommendations:\n",
    "        recommended_anime.add(recommendation)\n",
    "\n",
    "coverage = (len(recommended_anime) / len(anime_df['name'])) * 100\n",
    "\n",
    "print(f\"Coverage: {coverage:.2f}%\")\n",
    "# Coverage: 47.07% testing all anime titles -> 16mins "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coverage Testing -> K-means clustering Content-based filtering approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_anime_titles = list(anime_with_ratings_df['name'].unique())\n",
    "\n",
    "# set to store all recommended anime titles\n",
    "recommended_anime = set()\n",
    "\n",
    "for anime_title in all_anime_titles:\n",
    "    recommendations = combined_recommendations_with_clustering(anime_title)[1]\n",
    "    for recommendation in recommendations:\n",
    "        recommended_anime.add(recommendation)\n",
    "\n",
    "clustering_coverage = (len(recommended_anime) / len(anime_df['name'])) * 100\n",
    "\n",
    "print(f\"Clustering Coverage: {clustering_coverage:.2f}%\")\n",
    "\n",
    "# Clustering Coverage: 45.63% -> 373 minutes to execute"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coverage Testing -> K-means clustering with PCA Content-based filtering approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_anime_titles = list(anime_with_ratings_df['name'].unique())\n",
    "\n",
    "# set to store all recommended anime titles\n",
    "recommended_anime = set()\n",
    "\n",
    "for anime_title in all_anime_titles:\n",
    "    recommendations = combined_recommendations_with_clustering_and_pca(anime_title)[1]\n",
    "    for recommendation in recommendations:\n",
    "        recommended_anime.add(recommendation)\n",
    "\n",
    "pca_coverage = (len(recommended_anime) / len(anime_df['name'])) * 100\n",
    "\n",
    "print(f\"Clustering with PCA Coverage: {pca_coverage:.2f}%\")\n",
    "\n",
    "# Clustering with PCA Coverage: 52.04% -> over 10 hours to compute"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Website related functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendations = combined_recommendations('Death Note')[1]\n",
    "\n",
    "recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gather information/data needed for the frontend"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve Anime IDs (MAL ID) for all the recommendations to help fetch and display them using Jikan API on the website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ids_for_recommendations(recommendations):\n",
    "\n",
    "    anime_ids = []\n",
    "    for rec in recommendations:\n",
    "        id = anime_df.loc[anime_df['name'] == rec]['anime_id'].values[0]\n",
    "        anime_ids.append(id)\n",
    "\n",
    "    return anime_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = get_ids_for_recommendations(recommendations)\n",
    "\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendation_ids_df = pd.DataFrame(ids, columns=['mal_id'])\n",
    "\n",
    "recommendation_ids_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Restoring original anime_df to use columns dropped for recommendation purposes as information to be passed to the frontend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_website_anime_df():\n",
    "    website_anime_df = pd.read_csv(\"datasets/animes.csv\")\n",
    "\n",
    "    # reformat dataframe: removing NaN values and renaming columns, etc.\n",
    "    website_anime_df.rename(columns={'title': 'name'}, inplace=True)\n",
    "    \n",
    "    website_anime_df['name'] = website_anime_df['name'].apply(text_cleaning)\n",
    "\n",
    "    website_anime_df.rename(columns={'uid': 'anime_id', 'score': 'rating'}, inplace=True)\n",
    "    website_anime_df.episodes.replace({'Unknown':np.nan},inplace=True)\n",
    "\n",
    "    # fill NaN values for images with a default MAL picture:\n",
    "    website_anime_df.fillna('https://image.myanimelist.net/ui/OK6W_koKDTOqqqLDbIoPAiC8a86sHufn_jOI-JGtoCQ', inplace=True)\n",
    "\n",
    "    website_anime_df.drop_duplicates(subset=['name'], inplace=True)\n",
    "    website_anime_df.dropna(inplace=True)\n",
    "    website_anime_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # replace the characters \"[]'\" with an empty space as the genre column is already of type string\n",
    "    website_anime_df['genre'] = website_anime_df['genre'].str.replace(\"'\", \"\", regex=False)\n",
    "    website_anime_df['genre'] = website_anime_df['genre'].str.replace(\"[\", \"\", regex=False)\n",
    "    website_anime_df['genre'] = website_anime_df['genre'].str.replace(\"]\", \"\", regex=False)\n",
    "\n",
    "    return website_anime_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "website_anime_df = get_website_anime_df()\n",
    "\n",
    "website_anime_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gathering only the rows which are produced from the recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "website_recommendations_df = website_anime_df[website_anime_df['name'].isin(recommendations)] \n",
    "website_recommendations_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "website_recommendations_df.head(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to gather and return the required data to the frontend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_for_recommendations(recommendations):\n",
    "    global website_anime_df\n",
    "\n",
    "    # recommendations = request.args.get('query')\n",
    "    recommendations = recommendations.replace('%20', ' ')\n",
    "    recommendations = recommendations.split(',')\n",
    "\n",
    "    website_recommendations_df = website_anime_df[website_anime_df['name'].isin(recommendations)] \n",
    "\n",
    "    anime_ids = []\n",
    "    img_urls = []\n",
    "    mal_link = []\n",
    "    status = 200\n",
    "\n",
    "    for rec in recommendations:\n",
    "        anime_rec = website_recommendations_df.loc[website_recommendations_df['name'] == rec]\n",
    "        anime_ids.append(int(anime_rec['anime_id'].values[0]))\n",
    "        img_urls.append(str(anime_rec['img_url'].values[0]))\n",
    "        mal_link.append(str(anime_rec['link'].values[0]))\n",
    "\n",
    "    return anime_ids, img_urls, mal_link, status"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bd13d575caf8b70215954cabbb125610f9a3fd956dffe94b396ea02dd8e04b1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
